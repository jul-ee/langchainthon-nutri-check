# streamlit_rag_supplements.py
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â… . Import & í™˜ê²½ ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import os
import streamlit as st
from typing import List

from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories.streamlit import StreamlitChatMessageHistory

OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]
st.set_page_config(page_title="ì˜ì–‘ì œ Check ì±—ë´‡", page_icon="ğŸ’Š")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â…¡. PDF ë¡œë“œ & ë²¡í„°ìŠ¤í† ì–´
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource
def load_and_split_pdfs(folder_path: str):
    docs = []
    for f in os.listdir(folder_path):
        if f.endswith(".pdf"):
            docs.extend(PyPDFLoader(os.path.join(folder_path, f)).load_and_split())
    return docs


@st.cache_resource
def create_vector_store(_docs):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
    split_docs = text_splitter.split_documents(_docs)

    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-small",
        openai_api_key=OPENAI_API_KEY,
    )
    faiss_index = FAISS.from_documents(split_docs, embeddings)

    persist_dir = "./faiss_index"
    # ì €ì¥ -> faiss_index/ ìë™ ìƒì„±ë¨
    # : FAISSê°€ ìƒì„±Â·ì‚¬ìš©í•˜ëŠ” ë²¡í„° ì¸ë±ìŠ¤ íŒŒì¼ ë³´ê´€ í´ë”
    faiss_index.save_local(persist_dir)
    return faiss_index


@st.cache_resource
def get_vectorstore(_docs):
    persist_dir = "./faiss_index"
    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-small",
        openai_api_key=OPENAI_API_KEY,
    )

    if os.path.exists(os.path.join(persist_dir, "index.faiss")):   # FAISS ì €ì¥ íŒŒì¼
        return FAISS.load_local(persist_dir, embeddings)
    return create_vector_store(_docs)


def format_docs(docs) -> str:
    return "\n\n".join(doc.page_content for doc in docs)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â…¢. LangChain ì»´í¬ë„ŒíŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource
def initialize_components(selected_model):
    """ì˜ì–‘ì œ ë¬¸ì„œ ê¸°ë°˜ RAG ì²´ì¸ ì´ˆê¸°í™”"""
    pages = load_and_split_pdfs("./data/supplement_knowledge")
    vectorstore = get_vectorstore(pages)
    retriever = vectorstore.as_retriever()

    # 1) ì§ˆë¬¸ ì¬êµ¬ì„± í”„ë¡¬í”„íŠ¸(ë™ì¼)
    contextualize_q_system_prompt = (
        "Given a chat history and the latest user question relating to "
        "dietary supplements, diseases, or schedules, reformulate a standalone "
        "question. Do NOT answer the question."
    )
    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("history"),
            ("human", "{input}"),
        ]
    )

    # 2) Q&A í”„ë¡¬í”„íŠ¸(ë³€ê²½)
    qa_system_prompt = """ë‹¹ì‹ ì€ ê°œì¸ ë§ì¶¤ ì˜ì–‘ì œ ì½”ì¹˜ì…ë‹ˆë‹¤. ì•„ë˜ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ
[1] ì í•©í•œ **ì˜ì–‘ì œ ì¶”ì²œ Top 3** (ì œí’ˆëª…Â·ì£¼ìš”ì„±ë¶„Â·ì¶”ì²œì´ìœ )
[2] **í•˜ë£¨ ì„­ì·¨ ìŠ¤ì¼€ì¤„** (ì•„ì¹¨/ì ì‹¬/ì €ë…/ì·¨ì¹¨ ì „ ë“±, ìš©ëŸ‰Â·ì£¼ê¸°)
[3] **ì£¼ì˜Â·ê¸ˆê¸° ì‚¬í•­** (ì§ˆí™˜, ì•½ë¬¼, ìˆ Â·í¡ì—°Â·ì¹´í˜ì¸ ìƒí˜¸ì‘ìš©)
ì„ í‘œì™€ ë¦¬ìŠ¤íŠ¸ë¡œ í•œêµ­ì–´Â·ì¡´ëŒ“ë§ë¡œ ì œì‹œí•˜ì„¸ìš”.

ë°˜ë“œì‹œ ì œê³µëœ ë¬¸í—Œì—ì„œ ê·¼ê±°ë¥¼ ì°¾ì•„ ë°˜ì˜í•˜ê³ , ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µí•˜ì„¸ìš”.

{context}"""
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", qa_system_prompt),
            MessagesPlaceholder("history"),
            ("human", "{input}"),
        ]
    )

    llm = ChatOpenAI(model=selected_model, openai_api_key=OPENAI_API_KEY)
    history_aware_retriever = create_history_aware_retriever(
        llm, retriever, contextualize_q_prompt
    )
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
    return rag_chain


def build_user_query(disease: str, taking: str, prefers: str) -> str:
    """3ê°œ ì…ë ¥ì„ í•˜ë‚˜ì˜ ìŠ¤íƒ ë“œì–¼ë¡  ì§ˆë¬¸ìœ¼ë¡œ ë¬¶ì–´ ì¤Œ"""
    return (
        f"ì§ˆí™˜/ì¦ìƒ: {disease} | í˜„ì¬ ë³µìš©: {taking} | ì›í•˜ëŠ” íŠ¹ì§•: {prefers}.\n"
        "ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì í•©í•œ ì˜ì–‘ì œ ì¶”ì²œ, ì„­ì·¨ ìŠ¤ì¼€ì¤„, ì£¼ì˜ì‚¬í•­ì„ ì•Œë ¤ì¤˜."
    )

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â…£. Streamlit UI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.header("ğŸ’Š ì˜ì–‘ì œ Check RAG ì±—ë´‡")

model_option = st.selectbox("GPT ëª¨ë¸ ì„ íƒ", ("gpt-4o-mini", "gpt-3.5-turbo-0125"))
rag_chain = initialize_components(model_option)

chat_history = StreamlitChatMessageHistory(key="chat_messages")
conversational_rag_chain = RunnableWithMessageHistory(
    rag_chain,
    lambda session_id: chat_history,
    input_messages_key="input",
    history_messages_key="history",
    output_messages_key="answer",
)

# ì…ë ¥ í¼
with st.form("user_profile_form"):
    col1, col2 = st.columns(2)
    with col1:
        disease = st.text_area("â‘  í˜„ì¬ ì§ˆí™˜Â·ì¦ìƒ", placeholder="ì˜ˆ) ë§Œì„± ìœ„ì—¼, ê³ ì§€í˜ˆì¦")
        taking = st.text_area("â‘¡ í˜„ì¬ ë³µìš© ì¤‘ì¸ ì˜ì–‘ì œ/ì•½", placeholder="ì˜ˆ) ë°€í¬ì‹œìŠ¬, ì¢…í•©ë¹„íƒ€ë¯¼")
    with col2:
        prefers = st.text_area(
            "â‘¢ ì›í•˜ëŠ” ì˜ì–‘ì œ íŠ¹ì§•", placeholder="ì˜ˆ) í”¼ë¡œ íšŒë³µ, ê°„ ë³´í˜¸, ê°„ë‹¨í•œ í•˜ë£¨ í•œ ì•Œ"
        )
    submitted = st.form_submit_button("ì¶”ì²œë°›ê¸°")

# ê¸°ì¡´ ëŒ€í™” ì¶œë ¥
for m in chat_history.messages:
    st.chat_message(m.type).write(m.content)

# ì œì¶œ ì´ë²¤íŠ¸
if submitted:
    if not all([disease, taking, prefers]):
        st.warning("ì„¸ í•­ëª©ì„ ëª¨ë‘ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        st.stop()

    user_query = build_user_query(disease, taking, prefers)
    st.chat_message("human").write(user_query)

    with st.chat_message("ai"):
        with st.spinner("ë¶„ì„ ì¤‘..."):
            config = {"configurable": {"session_id": "any"}}
            response = conversational_rag_chain.invoke({"input": user_query}, config)
            answer = response["answer"]
            st.write(answer)

            with st.expander("ğŸ“š ì°¸ê³  ë¬¸í—Œ ë³´ê¸°"):
                for doc in response["context"]:
                    st.markdown(
                        f"**{os.path.basename(doc.metadata['source'])}**",
                        help=doc.page_content,
                    )

# ë©´ì±… ê³ ì§€
st.caption("âš ï¸ ë³¸ ì„œë¹„ìŠ¤ëŠ” ì¼ë°˜ì ì¸ ê±´ê°• ì •ë³´ ì œê³µì„ ëª©ì ìœ¼ë¡œ í•˜ë©°, "
           "ê°œì¸ ì²˜ë°©ì´ ì•„ë‹™ë‹ˆë‹¤. ë³µìš© ì „ ë°˜ë“œì‹œ ì „ë¬¸ê°€ì™€ ìƒë‹´í•˜ì„¸ìš”.")

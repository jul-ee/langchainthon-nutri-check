# streamlit_rag_supplements.py
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â… . Import & í™˜ê²½ ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import os
import streamlit as st
from typing import List

from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories.streamlit import StreamlitChatMessageHistory
from langchain.retrievers import EnsembleRetriever

OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]
st.set_page_config(page_title="ì˜ì–‘ì œ Check ì±—ë´‡", page_icon="ğŸ’Š")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â…¡. PDF ë¡œë“œ & ë²¡í„°ìŠ¤í† ì–´
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource
# PDFë¥¼ ì¹´í…Œê³ ë¦¬ í´ë” ë‹¨ìœ„ë¡œ ë¡œë“œí•˜ê³  LangChain Documentë¡œ ë³€í™˜
def load_and_split_pdfs_by_category(base_path: str) -> dict:
    all_docs_by_category = {}
    for category in os.listdir(base_path):
        category_path = os.path.join(base_path, category)
        if os.path.isdir(category_path):
            docs = []
            for f in os.listdir(category_path):
                if f.endswith(".pdf"):
                    docs.extend(PyPDFLoader(os.path.join(category_path, f)).load())
            all_docs_by_category[category] = docs
    return all_docs_by_category


@st.cache_resource
# ë¬¸ì„œë¥¼ chunk ë‹¨ìœ„ë¡œ ë¶„í•  (1000ì, ì¤‘ë³µ 150ì)
# OpenAI Embeddingìœ¼ë¡œ ë²¡í„° ìƒì„±
# FAISS ì¸ë±ìŠ¤ë¥¼ ì¹´í…Œê³ ë¦¬ ë‹¨ìœ„ë¡œ ì €ì¥í•˜ê±°ë‚˜ ë¡œë“œ
# -> ìš´ì˜ í™˜ê²½ ë°°í¬ ì‹œì—ëŠ” .pkl ì œê±° ë° faiss + json ë°©ì‹ ê¶Œì¥
def create_vectorstore_per_category(_docs_by_category: dict) -> dict:
    vectorstores = {}
    for category, docs in _docs_by_category.items():
        persist_dir = f"./faiss_index/{category}"
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small", openai_api_key=OPENAI_API_KEY)

        if os.path.exists(os.path.join(persist_dir, "index.faiss")):
            # ì´ë¯¸ ì €ì¥ëœ ì¸ë±ìŠ¤ê°€ ìˆë‹¤ë©´ ë¡œë“œ
            vectorstores[category] = FAISS.load_local(
                persist_dir,
                embeddings,
                allow_dangerous_deserialization=True  # ë³´ì•ˆì„ ìœ„í•´ ê¸°ë³¸ì ìœ¼ë¡œ Pickle ì‚¬ìš©ì„ ê¸ˆì§€
            )
        else:
            # ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„± í›„ ì €ì¥
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
            split_docs = text_splitter.split_documents(docs)
            faiss_index = FAISS.from_documents(split_docs, embeddings)
            faiss_index.save_local(persist_dir)
            vectorstores[category] = faiss_index

    return vectorstores

# ì‚¬ìš©ì ì¿¼ë¦¬ì— í¬í•¨ëœ í‚¤ì›Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ ì¹´í…Œê³ ë¦¬ ì„ íƒ
# ë‹¤ì¤‘ ì¹´í…Œê³ ë¦¬ ëŒ€ì‘ (Case A~D ì „ìš©)
def categorize_user_query(query: str) -> List[str]:
    """ì‹œì—°ìš©: ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ ì¼€ì´ìŠ¤ A~Dì— ë§ëŠ” í‚¤ì›Œë“œ ê¸°ë°˜ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ"""
    mapping = {
        "category_A": ["ìˆ ", "ë‹´ë°°", "ì»¤í”¼", "ëˆˆ ê±´ê°•", "ê°„", "í”¼ë¡œ íšŒë³µ"],
        "category_B": ["ë¹ˆí˜ˆ", "ìš´ë™", "ì‹ìŠµê´€", "ì² ë¶„ì œ", "ì¢…í•©ë¹„íƒ€ë¯¼", "ëˆˆ ê±´ê°•", "ê·¼ìœ¡ íšŒë³µ"],
        "category_C": ["ë‹¹ë‡¨ë³‘", "ê³ í˜ˆì••", "ì‹¬í˜ˆê´€", "í˜ˆì••"],
        "category_D": ["ê´€ì ˆì—¼", "ì¹˜ë§¤", "ë‡Œ ê±´ê°•", "ë‡Œ ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ"],
    }

    query_lower = query.lower()
    selected = []

    for category, keywords in mapping.items():
        if any(keyword.lower() in query_lower for keyword in keywords):
            selected.append(category)

    return list(set(selected))


# ì„ íƒëœ ì¹´í…Œê³ ë¦¬ì˜ retrieverë“¤ì„ EnsembleRetrieverë¡œ í†µí•©
# -> create_history_aware_retrieverë¥¼ í†µí•´ ëŒ€í™” ê¸°ë°˜ ì§ˆë¬¸ ë¦¬í¬ë§·
# -> QA Prompt êµ¬ì„± í›„ create_retrieval_chainìœ¼ë¡œ RAG ì™„ì„±
def build_rag_chain_from_categories(categories: List[str], vectorstores: dict):
    # combined_docs = []
    for cat in categories:
        retriever = vectorstores[cat].as_retriever()

    embeddings = OpenAIEmbeddings(model="text-embedding-3-small", openai_api_key=OPENAI_API_KEY)
    llm = ChatOpenAI(model="gpt-4o-mini", openai_api_key=OPENAI_API_KEY)
    retrievers = [vectorstores[cat].as_retriever() for cat in categories]
    retriever = EnsembleRetriever(retrievers=retrievers)

    contextualize_q_prompt = ChatPromptTemplate.from_messages([
        ("system", "Given a chat history and the latest user question relating to dietary supplements, diseases, or schedules, reformulate a standalone question. Do NOT answer the question."),
        MessagesPlaceholder("history"),
        ("human", "{input}"),
    ])

    qa_prompt = ChatPromptTemplate.from_messages([
        ("system",  """ë‹¹ì‹ ì€ ê°œì¸ ë§ì¶¤ ì˜ì–‘ì œ ì½”ì¹˜ì…ë‹ˆë‹¤. ì•„ë˜ ì‚¬ìš©ì ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, í•´ë‹¹ ì‚¬ìš©ìê°€ ì„­ì·¨í•´ë„ ë˜ëŠ” ì„±ë¶„ê³¼ í”¼í•´ì•¼ í•  ì„±ë¶„ì„ êµ¬ì²´ì ì´ê³  ëª…í™•í•˜ê²Œ ë¶„ë¥˜í•˜ì—¬ ì•Œë ¤ì£¼ì„¸ìš”.

[ë‹µë³€ êµ¬ì„± ë°©ì‹]
1. ì¶”ì²œ ì„±ë¶„ ëª©ë¡ (ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ëª©ì ì— ë¶€í•©í•˜ëŠ” ì˜ì–‘ì œ ì„±ë¶„)
    - ì„±ë¶„ëª…
    - ê¸°ëŠ¥ ë˜ëŠ” íš¨ëŠ¥
    - ì°¸ê³  ë¬¸í—Œ ì¶œì²˜(URL ë˜ëŠ” ì œëª©/ì €ë„/ì—°ë„)
2. í”¼í•´ì•¼ í•  ì„±ë¶„ ëª©ë¡ (ë³µìš© ì¤‘ì¸ ì•½ì œ, ì§ˆí™˜ ë“±ê³¼ ìƒí˜¸ì‘ìš© ìœ„í—˜ ìˆëŠ” ì„±ë¶„)
    - ì„±ë¶„ëª…
    - ìƒí˜¸ì‘ìš© ì›ì¸ ë˜ëŠ” ìœ„í—˜ ì´ìœ 
    - ê´€ë ¨ ë¬¸í—Œ ì¶œì²˜(URL ë˜ëŠ” ì œëª©/ì €ë„/ì—°ë„)
3. ì¶”ì²œ ì„±ë¶„ì˜ ì£¼ì˜ì‚¬í•­ ì„¤ëª…
â†’ 1ë²ˆì— í¬í•¨ëœ ê° ì„±ë¶„ë³„ë¡œ ì•Œë ˆë¥´ê¸° ìœ ë°œ, ê³ ìš©ëŸ‰ ë¶€ì‘ìš©, ì¥ê¸° ë³µìš© ì œí•œ, íŠ¹ì • ì—°ë ¹/ì„±ë³„ ê³ ë ¤ì‚¬í•­ ë“± ì£¼ì˜í•´ì•¼ í•  ì‚¬í•­ì„ ì¤„ê¸€ í˜•ì‹ìœ¼ë¡œ ê°„ëµíˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.

[ë‹µë³€ ì›ì¹™]
ì¶”ì²œ ì„±ë¶„ ëª©ë¡, í”¼í•´ì•¼ í•  ì„±ë¶„ ëª©ë¡ì€ ë¬¸í—Œì— ê·¼ê±°í•˜ì—¬ ì‘ì„±í•´ì•¼ í•˜ë©°, ëª…ì‹œì  ì¶œì²˜ë¥¼ ì œì‹œí•˜ì„¸ìš”.
ì¶œì²˜ ì—†ëŠ” ì£¼ì¥ì€ í—ˆìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
ë¬¸í—Œ ë‚´ ì–¸ê¸‰ì´ ì—†ê±°ë‚˜ ëª…í™•í•˜ì§€ ì•Šì€ ê²½ìš°, â€œì •ë³´ ë¶€ì¡±â€ ë˜ëŠ” â€œê·¼ê±° ì—†ìŒâ€ì´ë¼ê³  ëª…ì‹œí•´ì£¼ì„¸ìš”.
ë°˜ë“œì‹œ â€œì˜ì–‘ì œì˜ ì„±ë¶„â€ì— ì§‘ì¤‘í•˜ì—¬ ë¶„ì„í•˜ê³ , ì œí’ˆëª…ì´ë‚˜ ìƒí™œìŠµê´€ ìš”ì†Œ(ì˜ˆ: ì»¤í”¼, ì•Œì½”ì˜¬, ë…¹ì°¨ ë“±)ëŠ” ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.

{context}"""),
        MessagesPlaceholder("history"),
        ("human", "{input}"),
    ])

    history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
    return rag_chain


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â…¢. LangChain ì»´í¬ë„ŒíŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# @st.cache_resource
# : queryì— ë”°ë¼ ë™ì ìœ¼ë¡œ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ê³ ë¥´ê¸° ë•Œë¬¸ì— ìºì‹œí•˜ë©´ ì´ì „ ì§ˆì˜ì™€ ê²°ê³¼ê°€ ì„ì—¬ ì˜ëª»ëœ ê²°ê³¼ë¥¼ ì¤„ ìˆ˜ ì„ìŒ
# RAG ì²´ì¸ ì´ˆê¸°í™”
# : ë¬¸ì„œ ë¡œë”© + ë²¡í„° ìƒì„±/ë¡œë“œ + ì¹´í…Œê³ ë¦¬ ì„ íƒ + ì²´ì¸ ìƒì„±
def initialize_components( query: str):
    """ì˜ì–‘ì œ ë¬¸ì„œ ê¸°ë°˜ RAG ì²´ì¸ ì´ˆê¸°í™” (ì¹´í…Œê³ ë¦¬ ê¸°ë°˜)"""
    docs_by_cat = load_and_split_pdfs_by_category("./data/supplement_knowledge")
    vectorstores = create_vectorstore_per_category(docs_by_cat)
    categories = categorize_user_query(query)
    return build_rag_chain_from_categories(categories, vectorstores)


def build_user_query(age: int, gender: str, disease: str, taking: str, prefers: str) -> str:
    return (
        f"ë‹¤ìŒì€ ê°œì¸ ë§ì¶¤ ì˜ì–‘ì œë¥¼ ì¶”ì²œë°›ê³ ì í•˜ëŠ” ì‚¬ìš©ì ì •ë³´ì…ë‹ˆë‹¤.\n\n"
        f"[ì‚¬ìš©ì ì •ë³´]\n"
        f"- ë‚˜ì´/ì„±ë³„: {age}ì„¸ {gender}\n"
        f"- ì§ˆí™˜/ì¦ìƒ/íŠ¹ì´ì‚¬í•­: {disease}\n"
        f"- í˜„ì¬ ë³µìš© ì¤‘ì¸ ì•½ì œ ë˜ëŠ” ì˜ì–‘ì œ: {taking}\n"
        f"- ì›í•˜ëŠ” ì˜ì–‘ì œ ê¸°ëŠ¥ ë˜ëŠ” íš¨ê³¼: {prefers}\n\n"
        "ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì–´ë–¤ ì„±ë¶„ì„ ì¶”ì²œí•˜ê±°ë‚˜ í”¼í•´ì•¼ í• ì§€ë¥¼ ë¶„ì„í•´ ì£¼ì„¸ìš”."
    )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â…£. Streamlit UI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.header("ğŸ’Š ì˜ì–‘ì œ Check RAG ì±—ë´‡")

chat_history = StreamlitChatMessageHistory(key="chat_messages")

# ì…ë ¥ í¼
with st.form("user_profile_form"):
    st.markdown("#### ğŸ‘¤ ì‚¬ìš©ì ê±´ê°• ì •ë³´ ì…ë ¥")

    # ì²« ì¤„: ì™¼ìª½ = ë‚˜ì´Â·ì„±ë³„, ì˜¤ë¥¸ìª½ = ì§ˆí™˜Â·ì¦ìƒ
    top_left, top_right = st.columns(2)
    with top_left:
        age = st.number_input("ë‚˜ì´", min_value=0, max_value=120, step=1, placeholder="ì˜ˆ: 35")
        gender = st.selectbox("ì„±ë³„", ["ì—¬ì„±", "ë‚¨ì„±", "ê¸°íƒ€"])
    with top_right:
        disease = st.text_area("â‘  ì§ˆí™˜Â·ì¦ìƒÂ·íŠ¹ì´ì‚¬í•­", placeholder="ì˜ˆ) ë§Œì„± ìœ„ì—¼, ê³ ì§€í˜ˆì¦")

    # ë‘ ë²ˆì§¸ ì¤„: ì™¼ìª½ = ë³µìš© ì¤‘, ì˜¤ë¥¸ìª½ = ì›í•˜ëŠ” íŠ¹ì§•
    bottom_left, bottom_right = st.columns(2)
    with bottom_left:
        taking = st.text_area("â‘¡ í˜„ì¬ ë³µìš© ì¤‘ì¸ ì˜ì–‘ì œÂ·ì•½í’ˆ", placeholder="ì˜ˆ) ì¢…í•©ë¹„íƒ€ë¯¼")
    with bottom_right:
        prefers = st.text_area("â‘¢ ì›í•˜ëŠ” ì˜ì–‘ì œ íŠ¹ì§•", placeholder="ì˜ˆ) í”¼ë¡œ íšŒë³µ, ê°„ ë³´í˜¸, í•˜ë£¨ í•œ ì•Œ")

    submitted = st.form_submit_button("ì¶”ì²œë°›ê¸°")

# ê¸°ì¡´ ëŒ€í™” ì¶œë ¥
for m in chat_history.messages:
    st.chat_message(m.type).write(m.content)

# ì œì¶œ ì´ë²¤íŠ¸
if submitted:
    if not all([age, gender, disease, taking, prefers]):
        st.warning("í•­ëª©ì„ ëª¨ë‘ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
        st.stop()

    user_query = build_user_query(age, gender, disease, taking, prefers)
    rag_chain = initialize_components(user_query)
    
    conversational_rag_chain = RunnableWithMessageHistory(
    rag_chain,
    lambda session_id: chat_history,
    input_messages_key="input",
    history_messages_key="history",
    output_messages_key="answer",
    )

    st.chat_message("human").write(user_query)

    with st.chat_message("ai"):
        with st.spinner("ë¶„ì„ ì¤‘..."):
            config = {"configurable": {"session_id": "any"}}
            response = conversational_rag_chain.invoke({"input": user_query}, config)
            answer = response["answer"]
            st.write(answer)

            with st.expander("ğŸ“š ì°¸ê³  ë¬¸í—Œ ë³´ê¸°"):
                for doc in response["context"]:
                    st.markdown(
                        f"**{os.path.basename(doc.metadata.get('source', 'ì¶œì²˜ ì—†ìŒ'))}**",
                        help=doc.page_content,
                    )

# ë©´ì±… ê³ ì§€
st.caption("âš ï¸ ë³¸ ì„œë¹„ìŠ¤ëŠ” ì¼ë°˜ì ì¸ ê±´ê°• ì •ë³´ ì œê³µì„ ëª©ì ìœ¼ë¡œ í•˜ë©°, "
           "ê°œì¸ ì²˜ë°©ì´ ì•„ë‹™ë‹ˆë‹¤. ë³µìš© ì „ ë°˜ë“œì‹œ ì „ë¬¸ê°€ì™€ ìƒë‹´í•˜ì„¸ìš”.")
